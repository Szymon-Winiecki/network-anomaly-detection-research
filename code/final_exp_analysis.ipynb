{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append('./datasets')\n",
    "sys.path.append('./models')\n",
    "sys.path.append('./utils')\n",
    "sys.path.append('./visualization')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils.experiment_utils import CSVLogger\n",
    "from visualization.comparison_plots import compare_with_literature, compare_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logs_dir = Path('../out/logs/final/')\n",
    "root_plots_dir = Path('../out/figures/final_commas/')\n",
    "\n",
    "datasets_order = ['KDDCUP99', 'UNSW-NB15', 'CTU-13_08', 'CTU-13_09', 'CTU-13_10', 'CTU-13_13']\n",
    "\n",
    "def handle_model(log_files, metric_field, model_name, literature_scores, literature_index, per_dataset_stats):\n",
    "\n",
    "    gr_dfs = []\n",
    "    for log_file in log_files:\n",
    "        logger = CSVLogger(root_logs_dir / log_file)\n",
    "        df = logger.get_df()\n",
    "\n",
    "        df = df[df['dataset_name'].isin(datasets_order)]\n",
    "\n",
    "        gr_df = df.groupby(['dataset_name'], as_index=False).agg({\n",
    "            'dataset_name': 'first',\n",
    "            metric_field: ['mean', 'std'],\n",
    "        })\n",
    "\n",
    "        gr_dfs.append(gr_df)\n",
    "\n",
    "    gr_df = pd.concat(gr_dfs, ignore_index=True)\n",
    "\n",
    "    names = gr_df['dataset_name'].squeeze().to_list()\n",
    "    means = gr_df[metric_field]['mean'].to_list()\n",
    "    stds = gr_df[metric_field]['std'].to_list()\n",
    "\n",
    "    sorting_indices = sorted(range(len(names)), key=lambda k: datasets_order.index(names[k]))\n",
    "    names = [names[i] for i in sorting_indices]\n",
    "    means = [means[i] for i in sorting_indices]\n",
    "    stds = [stds[i] for i in sorting_indices]\n",
    "\n",
    "\n",
    "    for idx, name in enumerate(names):\n",
    "        if name not in per_dataset_stats:\n",
    "            per_dataset_stats[name] = {'models': [], 'mean': [], 'std': []}\n",
    "\n",
    "        per_dataset_stats[name]['models'].append(model_name)\n",
    "        per_dataset_stats[name]['mean'].append(means[idx])\n",
    "        per_dataset_stats[name]['std'].append(stds[idx])\n",
    "\n",
    "    # Compare with literature if any literature scores are provided\n",
    "    if len([l for l in literature_scores if l is not None]) > 0:\n",
    "        compare_with_literature(means, stds, literature_scores, names, literature_index, model_name, save_path= root_plots_dir / f\"literature/{model_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_dataset_stats = {}\n",
    "\n",
    "handle_model([\"AE.csv\"], 'val_auroc', 'AE(RE)', [None, None, None, None, None, None], 0, per_dataset_stats)\n",
    "handle_model([\"CAE.csv\"], 'val_auroc', 'CAE(M-CEN)', [None, None, 0.994, 0.959, 0.996, 0.979], 3, per_dataset_stats)\n",
    "handle_model([\"SAE_cen.csv\"], 'val_auroc', 'SAE(CEN)', [None, 0.886, 0.991, 0.950, 0.999, 0.969], 1, per_dataset_stats)\n",
    "handle_model([\"SAE_svm.csv\"], 'val_auroc', 'SAE(OCSVM)', [None, 0.893, 0.990, 0.950, 0.999, 0.971], 1, per_dataset_stats)\n",
    "handle_model([\"SAE_lof.csv\"], 'val_auroc', 'SAE(LOF)', [None, 0.894, 0.983, 0.960, 1.000, 0.975], 1, per_dataset_stats)\n",
    "handle_model([\"KSAE_SEP_cen.csv\"], 'test_auroc', 'KSAE(CEN)', [None, 0.885, None, 0.946, 0.989, 0.962], 2, per_dataset_stats)\n",
    "handle_model([\"KSAE_SEP_svm.csv\", \"KSAE_SEP_svm_2.csv\"], 'test_auroc', 'KSAE(OCSVM)', [None, 0.885, None, 0.946, 0.989, 0.962], 2, per_dataset_stats)\n",
    "handle_model([\"KSAE_SEP_lof.csv\", \"KSAE_SEP_lof_2.csv\"], 'test_auroc', 'KSAE(LOF)', [None, 0.885, None, 0.946, 0.989, 0.962], 2, per_dataset_stats)\n",
    "handle_model([\"BAE_2.csv\"], 'test_auroc', 'BAE', [None, None, None, None, None, None], 4, per_dataset_stats)\n",
    "\n",
    "\n",
    "for name, stats in per_dataset_stats.items():\n",
    "    compare_models(name, stats['models'], stats['mean'], stats['std'], save_path = root_plots_dir / f\"models/{name}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(\"../out/logs/final/AE.csv\")\n",
    "\n",
    "df = logger.get_df()\n",
    "\n",
    "df.head()\n",
    "\n",
    "gr_df = df.groupby(['pack_id'], as_index=False).agg({\n",
    "    'dataset_name': 'first',\n",
    "    'val_auroc': ['mean', 'std'],\n",
    "    'val_mcc': ['mean', 'std'],\n",
    "    'hidden_sizes': 'first',\n",
    "    'initial_lr': 'first',\n",
    "    'fit_duration' : ['mean', 'std'],\n",
    "})\n",
    "\n",
    "\n",
    "names = gr_df['dataset_name'].squeeze().to_list()\n",
    "means = gr_df['val_auroc']['mean'].to_list()\n",
    "stds = gr_df['val_auroc']['std'].to_list()\n",
    "\n",
    "literature = [None, None, None, None, None, None]\n",
    "\n",
    "# compare_with_literature(means, stds,literature, names, 'AE(RE)', save_path='../out/figures/pre-final/AE.png')\n",
    "\n",
    "for idx, name in enumerate(names):\n",
    "    if name not in per_dataset_stats:\n",
    "        per_dataset_stats[name] = {'models': [], 'mean': [], 'std': []}\n",
    "\n",
    "    per_dataset_stats[name]['models'].append('AE(RE)')\n",
    "    per_dataset_stats[name]['mean'].append(means[idx])\n",
    "    per_dataset_stats[name]['std'].append(stds[idx])\n",
    "\n",
    "gr_df.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(\"../out/logs/final/CAE.csv\")\n",
    "\n",
    "df = logger.get_df()\n",
    "\n",
    "df.head()\n",
    "\n",
    "gr_df = df.groupby(['pack_id'], as_index=False).agg({\n",
    "    'dataset_name': 'first',\n",
    "    'val_auroc': ['mean', 'std'],\n",
    "    'val_mcc': ['mean', 'std'],\n",
    "    'hidden_sizes': 'first',\n",
    "    'initial_lr': 'first',\n",
    "    'fit_duration' : ['mean', 'std'],\n",
    "})\n",
    "\n",
    "\n",
    "names = gr_df['dataset_name'].squeeze().to_list()\n",
    "means = gr_df['val_auroc']['mean'].to_list()\n",
    "stds = gr_df['val_auroc']['std'].to_list()\n",
    "\n",
    "literature = [None, None, 0.994, 0.959, 0.996, 0.979]\n",
    "\n",
    "for idx, name in enumerate(names):\n",
    "    if name not in per_dataset_stats:\n",
    "        per_dataset_stats[name] = {'models': [], 'mean': [], 'std': []}\n",
    "\n",
    "    per_dataset_stats[name]['models'].append('CAE(M-CEN)')\n",
    "    per_dataset_stats[name]['mean'].append(means[idx])\n",
    "    per_dataset_stats[name]['std'].append(stds[idx])\n",
    "\n",
    "\n",
    "compare_with_literature(means, stds, literature, names, 39, 'CAE(M-CEN)', save_path='../out/figures/final_commas/literature/CAE(M-CEN).png')\n",
    "\n",
    "gr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(\"../out/logs/final/SAE_lof.csv\")\n",
    "\n",
    "df = logger.get_df()\n",
    "\n",
    "df.head()\n",
    "\n",
    "gr_df = df.groupby(['pack_id'], as_index=False).agg({\n",
    "    'dataset_name': 'first',\n",
    "    'val_auroc': ['mean', 'std'],\n",
    "    'hidden_sizes': 'first',\n",
    "    'initial_lr': 'first',\n",
    "    'fit_duration' : ['mean', 'std'],\n",
    "})\n",
    "\n",
    "\n",
    "names = gr_df['dataset_name'].squeeze().to_list()\n",
    "means = gr_df['val_auroc']['mean'].to_list()\n",
    "stds = gr_df['val_auroc']['std'].to_list()\n",
    "\n",
    "literature = [None, 0.894, 0.983, 0.960, 1.000, 0.975]\n",
    "\n",
    "for idx, name in enumerate(names):\n",
    "    if name not in per_dataset_stats:\n",
    "        per_dataset_stats[name] = {'models': [], 'mean': [], 'std': []}\n",
    "\n",
    "    per_dataset_stats[name]['models'].append('SAE(LOF)')\n",
    "    per_dataset_stats[name]['mean'].append(means[idx])\n",
    "    per_dataset_stats[name]['std'].append(stds[idx])\n",
    "\n",
    "compare_with_literature(means, stds, literature, names, 8, 'SAE(LOF)', save_path='../out/figures/final_commas/literature/SAE(LOF).png')\n",
    "\n",
    "gr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(\"../out/logs/final/SAE_cen.csv\")\n",
    "\n",
    "df = logger.get_df()\n",
    "\n",
    "df.head()\n",
    "\n",
    "gr_df = df.groupby(['pack_id'], as_index=False).agg({\n",
    "    'dataset_name': 'first',\n",
    "    'val_auroc': ['mean', 'std'],\n",
    "    'val_mcc': ['mean', 'std'],\n",
    "    'hidden_sizes': 'first',\n",
    "    'initial_lr': 'first',\n",
    "    'fit_duration' : ['mean', 'std'],\n",
    "})\n",
    "\n",
    "\n",
    "names = gr_df['dataset_name'].squeeze().to_list()\n",
    "means = gr_df['val_auroc']['mean'].to_list()\n",
    "stds = gr_df['val_auroc']['std'].to_list()\n",
    "\n",
    "literature = [None, 0.886, 0.991, 0.950, 0.999, 0.969]\n",
    "\n",
    "for idx, name in enumerate(names):\n",
    "    if name not in per_dataset_stats:\n",
    "        per_dataset_stats[name] = {'models': [], 'mean': [], 'std': []}\n",
    "\n",
    "    per_dataset_stats[name]['models'].append('SAE(CEN)')\n",
    "    per_dataset_stats[name]['mean'].append(means[idx])\n",
    "    per_dataset_stats[name]['std'].append(stds[idx])\n",
    "\n",
    "# compare_with_literature(means, stds, literature, names, 8, 'SAE(CEN)', save_path='../out/figures/final/literature/SAE(CEN).png')\n",
    "\n",
    "gr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(\"../out/logs/final/SAE_svm.csv\")\n",
    "\n",
    "df = logger.get_df()\n",
    "\n",
    "df.head()\n",
    "\n",
    "gr_df = df.groupby(['pack_id'], as_index=False).agg({\n",
    "    'dataset_name': 'first',\n",
    "    'val_auroc': ['mean', 'std'],\n",
    "    'hidden_sizes': 'first',\n",
    "    'initial_lr': 'first',\n",
    "    'fit_duration' : ['mean', 'std'],\n",
    "})\n",
    "\n",
    "\n",
    "names = gr_df['dataset_name'].squeeze().to_list()\n",
    "means = gr_df['val_auroc']['mean'].to_list()\n",
    "stds = gr_df['val_auroc']['std'].to_list()\n",
    "\n",
    "literature = [None, 0.893, 0.990, 0.950, 0.999, 0.971]\n",
    "\n",
    "for idx, name in enumerate(names):\n",
    "    if name not in per_dataset_stats:\n",
    "        per_dataset_stats[name] = {'models': [], 'mean': [], 'std': []}\n",
    "\n",
    "    per_dataset_stats[name]['models'].append('SAE(OCSVM)')\n",
    "    per_dataset_stats[name]['mean'].append(means[idx])\n",
    "    per_dataset_stats[name]['std'].append(stds[idx])\n",
    "\n",
    "# compare_with_literature(means, stds, literature, names, 8, 'SAE(OCSVM)', save_path='../out/figures/final/literature/SAE(OCSVM).png')\n",
    "\n",
    "gr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(\"../out/logs/final/KSAE_SEP_cen.csv\")\n",
    "\n",
    "df = logger.get_df()\n",
    "\n",
    "df.head()\n",
    "\n",
    "gr_df = df.groupby(['pack_id'], as_index=False).agg({\n",
    "    'dataset_name': 'first',\n",
    "    'test_auroc': ['mean', 'std'],\n",
    "    'fit_duration' : ['mean', 'std'],\n",
    "})\n",
    "\n",
    "\n",
    "names = gr_df['dataset_name'].squeeze().to_list()\n",
    "means = gr_df['test_auroc']['mean'].to_list()\n",
    "stds = gr_df['test_auroc']['std'].to_list()\n",
    "\n",
    "literature = [None, 0.885, None, 0.946, 0.989, 0.962]\n",
    "\n",
    "for idx, name in enumerate(names):\n",
    "    if name not in per_dataset_stats:\n",
    "        per_dataset_stats[name] = {'models': [], 'mean': [], 'std': []}\n",
    "\n",
    "    per_dataset_stats[name]['models'].append('KSAE(CEN)')\n",
    "    per_dataset_stats[name]['mean'].append(means[idx])\n",
    "    per_dataset_stats[name]['std'].append(stds[idx])\n",
    "\n",
    "# compare_with_literature(means, stds, literature, names, 5, 'KSAE(CEN)', save_path='../out/figures/final/literature/KSAE(CEN).png')\n",
    "\n",
    "gr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(\"../out/logs/final/KSAE_SEP_lof.csv\")\n",
    "logger2 = CSVLogger(\"../out/logs/final/KSAE_SEP_lof_2.csv\")\n",
    "\n",
    "df = logger.get_df()\n",
    "df2 = logger2.get_df()\n",
    "\n",
    "\n",
    "gr_df = df.groupby(['dataset_name'], as_index=False).agg({\n",
    "    'dataset_name': 'first',\n",
    "    'test_auroc': ['mean', 'std'],\n",
    "    'fit_duration' : ['mean', 'std'],\n",
    "})\n",
    "\n",
    "gr_df2 = df2.groupby(['dataset_name'], as_index=False).agg({\n",
    "    'dataset_name': 'first',\n",
    "    'test_auroc': ['mean', 'std'],\n",
    "    'fit_duration' : ['mean', 'std'],\n",
    "})\n",
    "\n",
    "gr_df = pd.concat([gr_df, gr_df2], ignore_index=True)\n",
    "\n",
    "\n",
    "gr_df = gr_df.iloc[[3, 4, 0, 5, 1, 2]]\n",
    "\n",
    "\n",
    "names = gr_df['dataset_name'].squeeze().to_list()\n",
    "means = gr_df['test_auroc']['mean'].to_list()\n",
    "stds = gr_df['test_auroc']['std'].to_list()\n",
    "\n",
    "literature = [None, 0.885, None, 0.946, 0.989, 0.962]\n",
    "\n",
    "for idx, name in enumerate(names):\n",
    "    if name not in per_dataset_stats:\n",
    "        per_dataset_stats[name] = {'models': [], 'mean': [], 'std': []}\n",
    "\n",
    "    per_dataset_stats[name]['models'].append('KSAE(LOF)')\n",
    "    per_dataset_stats[name]['mean'].append(means[idx])\n",
    "    per_dataset_stats[name]['std'].append(stds[idx])\n",
    "\n",
    "# compare_with_literature(means, stds, literature, names, 5, 'KSAE(LOF)', save_path='../out/figures/final/literature/KSAE(SEP,LOF).png')\n",
    "\n",
    "gr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(\"../out/logs/final/KSAE_SEP_svm.csv\")\n",
    "logger2 = CSVLogger(\"../out/logs/final/KSAE_SEP_svm_2.csv\")\n",
    "\n",
    "df = logger.get_df()\n",
    "df2 = logger2.get_df()\n",
    "\n",
    "gr_df = df.groupby(['dataset_name'], as_index=False).agg({\n",
    "    'dataset_name': 'first',\n",
    "    'test_auroc': ['mean', 'std'],\n",
    "    'fit_duration' : ['mean', 'std'],\n",
    "})\n",
    "\n",
    "gr_df2 = df2.groupby(['dataset_name'], as_index=False).agg({\n",
    "    'dataset_name': 'first',\n",
    "    'test_auroc': ['mean', 'std'],\n",
    "    'fit_duration' : ['mean', 'std'],\n",
    "})\n",
    "\n",
    "gr_df = pd.concat([gr_df, gr_df2], ignore_index=True)\n",
    "\n",
    "gr_df = gr_df.iloc[[4, 5, 0, 6, 2, 3]]\n",
    "\n",
    "names = gr_df['dataset_name'].squeeze().to_list()\n",
    "means = gr_df['test_auroc']['mean'].to_list()\n",
    "stds = gr_df['test_auroc']['std'].to_list()\n",
    "\n",
    "literature = [None, 0.885, None, 0.946, 0.989, 0.962]\n",
    "\n",
    "for idx, name in enumerate(names):\n",
    "    if name not in per_dataset_stats:\n",
    "        per_dataset_stats[name] = {'models': [], 'mean': [], 'std': []}\n",
    "\n",
    "    per_dataset_stats[name]['models'].append('KSAE(OCSVM)')\n",
    "    per_dataset_stats[name]['mean'].append(means[idx])\n",
    "    per_dataset_stats[name]['std'].append(stds[idx])\n",
    "\n",
    "# compare_with_literature(means, stds, literature, names, 5, 'KSAE(OCSVM)', save_path='../out/figures/final/literature/KSAE(OCSVM).png')\n",
    "\n",
    "gr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(\"../out/logs/final/BAE_2.csv\")\n",
    "\n",
    "df = logger.get_df()\n",
    "\n",
    "df.head()\n",
    "\n",
    "gr_df = df.groupby(['pack_id'], as_index=False).agg({\n",
    "    'dataset_name': 'first',\n",
    "    'test_auroc': ['mean', 'std'],\n",
    "    'fit_duration' : ['mean', 'std'],\n",
    "})\n",
    "\n",
    "\n",
    "names = gr_df['dataset_name'].squeeze().to_list()\n",
    "means = gr_df['test_auroc']['mean'].to_list()\n",
    "stds = gr_df['test_auroc']['std'].to_list()\n",
    "\n",
    "literature = [None, None, None, None, None, None]\n",
    "\n",
    "for idx, name in enumerate(names):\n",
    "    if name not in per_dataset_stats:\n",
    "        per_dataset_stats[name] = {'models': [], 'mean': [], 'std': []}\n",
    "\n",
    "    per_dataset_stats[name]['models'].append('BAE')\n",
    "    per_dataset_stats[name]['mean'].append(means[idx])\n",
    "    per_dataset_stats[name]['std'].append(stds[idx])\n",
    "\n",
    "# compare_with_literature(means, stds, literature, names, 56, 'BAE', save_path='../out/figures/final/literature/BAE.png')\n",
    "\n",
    "gr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, stats in per_dataset_stats.items():\n",
    "    compare_models(name, stats['models'], stats['mean'], stats['std'], save_path=f'../out/figures/final/models/{name}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
